{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep for Rasa blog post\n",
    "\n",
    "This notebook outlines the preliminary data prep done for my blog post on creating a medical diagnosis bot using rasa.\n",
    "\n",
    "Part 1 describes how to prepare the input data for the diagnosis portion of the project while part 2 describes how I generated some training data for Rasa's NLU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Vector data prep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Here we just have th one function `get_sentence_vector`. This function simply takes some text and a spacy nlp object and converts the text to a 246D vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectors(text, nlp):\n",
    "    \n",
    "    # get tokens for each word in sentence\n",
    "    embedding = nlp(text).vector.tolist()\n",
    "    \n",
    "    # return mean token\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and merge data\n",
    "\n",
    "We only care about 3 files downloaded from Kaggle `dia_t.csv` (list of illnesses with id's), `sym_t.csv` (list of symptoms with id's), and `diffsydiw.csv` (a mapping of illness id's to various symptom id's).\n",
    "\n",
    "once imported we can join everyhing together on the ID columns and do some basic cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df = pd.read_csv('data/dia_t.csv')\n",
    "symptom_df = pd.read_csv('data/sym_t.csv')\n",
    "links_df = pd.read_csv('data/diffsydiw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = (links_df\n",
    "               .merge(illness_df, on=\"did\")\n",
    "               .merge(symptom_df, on=\"syd\"))\n",
    "\n",
    "# remove any missing data and select columns we need\n",
    "source_data = source_data.loc[~(source_data['symptom'].isna())\n",
    "                             & ~(source_data['diagnose'].isna()),\n",
    "                             ['did', 'syd', 'diagnose', 'symptom']]\n",
    "source_data.columns = ['illness_id', 'symptom_id', 'illness', 'symptom']\n",
    "\n",
    "\n",
    "# tidy up some new messy characters\n",
    "source_data['illness'] = source_data['illness'].str.replace('\\x0b', ' ')\n",
    "source_data['symptom'] = source_data['symptom'].str.replace('\\x0b', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our function to convert each symptom to a vector representation we can then save our symptom data down to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_df = symptom_df.loc[~symptom_df['symptom'].isna()]\n",
    "symptom_df['embedding'] = symptom_df.apply(lambda row: get_sentence_vectors(row['symptom'], nlp), axis = 1)\n",
    "symptom_df.columns = ['symptom_id', 'symptom', 'symptom_vector']\n",
    "\n",
    "# remove any messy characters\n",
    "symptom_df['symptom'] = symptom_df['symptom'].str.replace('\\x0b', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data.to_pickle('data/source_data.pkl')\n",
    "symptom_df.to_pickle('data/symptoms.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vector of symtpoms for each illness described in the data\n",
    "\n",
    "Here we will loop through each illness described in the dataset and flag (with a 1 or 0) each of the symptoms of that illness. We will do then compare a list of flagged symptoms described by the user to this using a cosine similarity function to diagnose a potential illness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ist of illness\n",
    "illnesses = list(source_data['illness'].drop_duplicates())\n",
    "\n",
    "# list we will use to store our illness vectors\n",
    "symptom_vectors = []\n",
    "\n",
    "for illness in illnesses:\n",
    "    \n",
    "    illness_symptoms = list(source_data.loc[source_data[\"illness\"]==illness, 'symptom'].drop_duplicates())\n",
    "    \n",
    "    symptom_df[\"related_to_illness\"] = 0\n",
    "    symptom_df.loc[symptom_df[\"symptom\"].isin(illness_symptoms), \"related_to_illness\"] = 1\n",
    "    \n",
    "    \n",
    "    symptom_vectors.append(list(symptom_df[\"related_to_illness\"]))\n",
    "    \n",
    "diagnosis_data = pd.DataFrame({\"illness\":illnesses,\n",
    "                              \"illness_vector\": symptom_vectors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Generate training samples with flagged entities\n",
    "\n",
    "In this part we will attempt to generate some training examples for the NLU model to understand how our users will describe symtoms to the chat bot. To do this we simply loop through our symptoms, sometimes combining them before appending them to different beginnings/endings.\n",
    "\n",
    "We also make sure to tag any symptom describtions as a symptom entity, which will be understood by Rasa's NLU model. You can see this is accomplished by inserting '(symptom)' beside any mention of symptoms.\n",
    "\n",
    "The output of this is inserted into nlu.md within the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_symptoms = [1, 2, 3, 4]\n",
    "start_of_description = [\n",
    "    \"I have\",\n",
    "    \"I'm suffering from\",\n",
    "    \"I have really bad\",\n",
    "    \"My symptoms are\",\n",
    "    \"For the last few days I have had\",\n",
    "    \"My husband is suffering from\" ,\n",
    "    \"My wife is suffering from\",\n",
    "    \"My son is suffering from\",\n",
    "    \"My daughter is suffering from\",\n",
    "    \"My child is suffering from\",\n",
    "    \"I don't feel well, I have\"\n",
    "]\n",
    "\n",
    "# get some examples of users describing different numbers of syptoms\n",
    "for symptons_count in number_of_symptoms:\n",
    "    \n",
    "    # make 100 examples of each number of symptoms\n",
    "    for ex in range(1, 101):\n",
    "    \n",
    "        description_beginning = random.choice(start_of_description)\n",
    "        \n",
    "        # collect some random symtpoms\n",
    "        symptom_1 = symptom_df['symptom'].sample(1).iloc[0].lower()\n",
    "        symptom_2 = symptom_df['symptom'].sample(1).iloc[0].lower()\n",
    "        symptom_3 = symptom_df['symptom'].sample(1).iloc[0].lower()\n",
    "        symptom_4 = symptom_df['symptom'].sample(1).iloc[0].lower()\n",
    "        \n",
    "        symptoms = [symptom_1, symptom_2, symptom_3, symptom_4]\n",
    "        symptoms_entity = []\n",
    "        \n",
    "        # remove parenthases from symptoms and add nessecary entitiy tags to symptoms\n",
    "        for symptom in symptoms:\n",
    "            symptom = re.sub(r\"\\([^)]+\\)\", \"\", symptom).strip()\n",
    "            symptom = f\"[{symptom}](symptom)\"\n",
    "            symptoms_entity.append(symptom)\n",
    "            \n",
    "        symptom_1 = symptoms_entity[0]\n",
    "        symptom_2 = symptoms_entity[1]\n",
    "        symptom_3 = symptoms_entity[2]\n",
    "        symptom_4 = symptoms_entity[3]\n",
    "        \n",
    "        # create the training sample string\n",
    "        if symptons_count == 1:\n",
    "            \n",
    "            symptom_string = f\"- {description_beginning} {symptom_1}\"\n",
    "            \n",
    "        if symptons_count == 2:\n",
    "            \n",
    "            symptom_string = f\"- {description_beginning} {symptom_1} and {symptom_2}\"\n",
    "            \n",
    "        if symptons_count == 3:\n",
    "            \n",
    "            symptom_string = f\"- {description_beginning} {symptom_1}, {symptom_2}, and {symptom_3}\"\n",
    "            \n",
    "        if symptons_count == 4:\n",
    "            \n",
    "            symptom_string = f\"- {description_beginning} {symptom_1}, {symptom_2}, {symptom_3}, {symptom_4}\"\n",
    "        \n",
    "        print(symptom_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
